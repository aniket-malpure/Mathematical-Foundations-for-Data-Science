{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Numpy is used to store the data in the form of arrays and apply some mathematical functionality to it\n",
        "import numpy as np\n",
        "# Counter in collections is used to store the number of iterable objects in the form of dictionary\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "dmF5tPvOgSth"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a node for a tree\n",
        "class Node:\n",
        "    # By default all the attributes are passed as None\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, current=None):\n",
        "        # Index value of the feature which is been used for splitting\n",
        "        self.feature = feature\n",
        "        # Threshold using which we are going to split the data\n",
        "        self.threshold = threshold\n",
        "        # References to the left child node\n",
        "        self.left = left\n",
        "        # References to the right child node\n",
        "        self.right = right\n",
        "        # Current predicted class\n",
        "        self.current = current"
      ],
      "metadata": {
        "id": "7DKycDfogyaO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main class where we are going to prepare a decision tree\n",
        "class DecisionTree:\n",
        "    # the init function will have all the hyperparameters\n",
        "    def __init__(self, maxDepth=100, minSplit=2):\n",
        "        # Define the maximum depth of the tree just to avoid overfitting\n",
        "        self.maxDepth = maxDepth\n",
        "        # Minimum number of samples required so perform split operation\n",
        "        self.minSplit = minSplit\n",
        "        # The root node of the tree\n",
        "        self.root = None\n",
        "\n",
        "    # Method to initiate the building of the tree\n",
        "    def fit(self, X, y):\n",
        "        # The root gets the first node\n",
        "        self.root = self.growTree(X, y)\n",
        "        print(\"\\nFinal Decision Tree Structure:\")\n",
        "        self.printTree(self.root)\n",
        "\n",
        "    # Method to build the tree\n",
        "    def growTree(self, X, y, depth=0):\n",
        "        # Number if total samples and features present\n",
        "        nSamples, nFeatures = X.shape\n",
        "        # Number of unique output present\n",
        "        nLabels = len(np.unique(y))\n",
        "        print(f\"\\n{'  ' * depth}Depth {depth}:\")\n",
        "        print(f\"{'  ' * depth}Labels: {y}\")\n",
        "\n",
        "        # Print entropy of the current node\n",
        "        nodeEntropy = self.entropy(y)\n",
        "        print(f\"{'  ' * depth}Node Entropy: {nodeEntropy:.4f}\")\n",
        "\n",
        "        # Stopping criteria\n",
        "        # 1. First condition checks whether the tree contructed has reached to its maximum depth\n",
        "        # 2. Second condition checks whether the node is pure or not i.e., only one possible outcome\n",
        "        # 3. Third condition checks if the number of sample in the given node are less than minimum of samples defined\n",
        "        if (depth >= self.maxDepth or nLabels == 1 or nSamples < self.minSplit):\n",
        "            # It will create a dictionary with unique outputs as keys and their total number of occurences as the values\n",
        "            counter = Counter(y)\n",
        "            # Extract the key with largest value assigned to it\n",
        "            leafValue = counter.most_common(1)[0][0]\n",
        "            print(f\"{'  ' * depth}Creating leaf node with value: {leafValue}\")\n",
        "            # Create a leaf node using it\n",
        "            return Node(current=leafValue)\n",
        "\n",
        "        # It allows to arrange the features randomly to avoid bias and replace = False takes care of repitition\n",
        "        featureIndexes = np.random.choice(nFeatures, nFeatures, replace=False)\n",
        "\n",
        "        # Find the best split\n",
        "        bestFeature, bestThreshold, bestGain = self.bestSplit(X, y, featureIndexes)\n",
        "\n",
        "        print(f\"{'  ' * depth}Best Split:\")\n",
        "        print(f\"{'  ' * depth}Feature: {bestFeature}, Threshold: {bestThreshold:.4f}\")\n",
        "        print(f\"{'  ' * depth}Information Gain: {bestGain:.4f}\")\n",
        "\n",
        "        # Create child nodes\n",
        "        leftIndexes, rightIndexes = self.split(X[:, bestFeature], bestThreshold)\n",
        "        # The depth + 1 indicated that these new nodes are one level deeper than their parent\n",
        "        print(f\"\\n{'  ' * depth}Left split: {len(leftIndexes)} samples\")\n",
        "        left = self.growTree(X[leftIndexes, :], y[leftIndexes], depth + 1)\n",
        "        print(f\"\\n{'  ' * depth}Right split: {len(rightIndexes)} samples\")\n",
        "        right = self.growTree(X[rightIndexes, :], y[rightIndexes], depth + 1)\n",
        "\n",
        "        # Create a node with left and right child\n",
        "        return Node(bestFeature, bestThreshold, left, right)\n",
        "\n",
        "    # Method to find the best split\n",
        "    def bestSplit(self, X, y, featureIndexes):\n",
        "        # Initialize a variable to store best possible gain\n",
        "        bestGain = -1\n",
        "        # Index of the feature used to split and the respective best threshold value\n",
        "        splitIndex, splitThreshold = None, None\n",
        "\n",
        "        # Traversing through each feature\n",
        "        for featureIndex in featureIndexes:\n",
        "            # Getting all the samples of the selected feature\n",
        "            XColumn = X[:, featureIndex]\n",
        "            # Creating an array to store all the unique value of the feature\n",
        "            thresholds = np.unique(XColumn)\n",
        "\n",
        "            # Iterating through each value in the above array to find the best threshold value\n",
        "            for threshold in thresholds:\n",
        "                # calculating gain using the selected threshold value\n",
        "                gain = self.informationGain(y, XColumn, threshold)\n",
        "\n",
        "                # Identifying best gain, its index and the threshold used for it\n",
        "                if gain > bestGain:\n",
        "                    bestGain = gain\n",
        "                    splitIndex = featureIndex\n",
        "                    splitThreshold = threshold\n",
        "\n",
        "        # Returning the index used to split and its best threshold\n",
        "        return splitIndex, splitThreshold, bestGain\n",
        "\n",
        "    # Method to find information gain\n",
        "    def informationGain(self, y, XColumn, threshold):\n",
        "        # Calculating entropy of the parent node\n",
        "        parentEntropy = self.entropy(y)\n",
        "\n",
        "        # Split the feature sample based on the threshold\n",
        "        leftIndex, rightIndex = self.split(XColumn, threshold)\n",
        "\n",
        "        # Checking whether there are elements in both the variables\n",
        "        if len(leftIndex) == 0 or len(rightIndex) == 0:\n",
        "            return 0\n",
        "\n",
        "        # Length of total sample\n",
        "        n = len(y)\n",
        "        # Length of total sample on left and right respectively\n",
        "        nLeft, nRight = len(leftIndex), len(rightIndex)\n",
        "        # Entropy of left child and right child\n",
        "        eLeft, eRight = self.entropy(y[leftIndex]), self.entropy(y[rightIndex])\n",
        "        # Calculating the child entropy\n",
        "        childEntropy = (nLeft / n) * eLeft + (nRight / n) * eRight\n",
        "        # returning information gain which is difference of entropy of parent and child node\n",
        "        return parentEntropy - childEntropy\n",
        "\n",
        "    # Method to split the samples based on the selected threshold value\n",
        "    def split(self, XColumn, splitThreshold):\n",
        "        # Storing values less than or equal to threshold value on left\n",
        "        leftindexes = np.argwhere(XColumn <= splitThreshold).flatten()\n",
        "        # Storing values greater than the threshold value on right\n",
        "        rightindexes = np.argwhere(XColumn > splitThreshold).flatten()\n",
        "        # Returning them\n",
        "        return leftindexes, rightindexes\n",
        "\n",
        "    # Method to calculate entropy\n",
        "    def entropy(self, y):\n",
        "        # Count the occurrences of each integer in the array\n",
        "        hist = np.bincount(y)\n",
        "        # Calculate the probability of each class\n",
        "        ps = hist / len(y)\n",
        "        # Using the formula of entropy we calculate it (H = -sum(p.log2(p)))\n",
        "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
        "\n",
        "    # Method to print the decision tree\n",
        "    def printTree(self, node, depth=0):\n",
        "        if node.current is not None:\n",
        "            print(f\"{'  ' * depth}Leaf: {node.current}\")\n",
        "        else:\n",
        "            print(f\"{'  ' * depth}[X{node.feature} <= {node.threshold:.4f}]\")\n",
        "            print(f\"{'  ' * depth}Left:\")\n",
        "            self.printTree(node.left, depth + 1)\n",
        "            print(f\"{'  ' * depth}Right:\")\n",
        "            self.printTree(node.right, depth + 1)\n",
        "\n",
        "    # Method to predict output for test data\n",
        "    def predict(self, X):\n",
        "        return np.array([self.traverseTree(x, self.root) for x in X])\n",
        "\n",
        "    # Method to traverse the input data\n",
        "    def traverseTree(self, x, node):\n",
        "        if node.current is not None:\n",
        "            return node.current\n",
        "\n",
        "        # Applying condition based on the selected threshold and feature\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self.traverseTree(x, node.left)\n",
        "        return self.traverseTree(x, node.right)"
      ],
      "metadata": {
        "id": "jiLndc3BgtBN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate a simple dataset\n",
        "    X = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]])\n",
        "    y = np.array([0, 1, 0, 1, 1, 0])\n",
        "\n",
        "    # Create and train the decision tree\n",
        "    tree = DecisionTree(maxDepth=3)\n",
        "    tree.fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    X_test = np.array([[25, 26, 27, 28], [29, 30, 31, 32]])\n",
        "    predictions = tree.predict(X_test)\n",
        "    print(\"\\nPredictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfcRHTKKgeUo",
        "outputId": "bcd0453e-3e13-4d3b-ba65-7d3b7c938e47",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Depth 0:\n",
            "Labels: [0 1 0 1 1 0]\n",
            "Node Entropy: 1.0000\n",
            "Best Split:\n",
            "Feature: 2, Threshold: 3.0000\n",
            "Information Gain: 0.1909\n",
            "\n",
            "Left split: 1 samples\n",
            "\n",
            "  Depth 1:\n",
            "  Labels: [0]\n",
            "  Node Entropy: -0.0000\n",
            "  Creating leaf node with value: 0\n",
            "\n",
            "Right split: 5 samples\n",
            "\n",
            "  Depth 1:\n",
            "  Labels: [1 0 1 1 0]\n",
            "  Node Entropy: 0.9710\n",
            "  Best Split:\n",
            "  Feature: 1, Threshold: 18.0000\n",
            "  Information Gain: 0.3219\n",
            "\n",
            "  Left split: 4 samples\n",
            "\n",
            "    Depth 2:\n",
            "    Labels: [1 0 1 1]\n",
            "    Node Entropy: 0.8113\n",
            "    Best Split:\n",
            "    Feature: 2, Threshold: 11.0000\n",
            "    Information Gain: 0.3113\n",
            "\n",
            "    Left split: 2 samples\n",
            "\n",
            "      Depth 3:\n",
            "      Labels: [1 0]\n",
            "      Node Entropy: 1.0000\n",
            "      Creating leaf node with value: 1\n",
            "\n",
            "    Right split: 2 samples\n",
            "\n",
            "      Depth 3:\n",
            "      Labels: [1 1]\n",
            "      Node Entropy: -0.0000\n",
            "      Creating leaf node with value: 1\n",
            "\n",
            "  Right split: 1 samples\n",
            "\n",
            "    Depth 2:\n",
            "    Labels: [0]\n",
            "    Node Entropy: -0.0000\n",
            "    Creating leaf node with value: 0\n",
            "\n",
            "Final Decision Tree Structure:\n",
            "[X2 <= 3.0000]\n",
            "Left:\n",
            "  Leaf: 0\n",
            "Right:\n",
            "  [X1 <= 18.0000]\n",
            "  Left:\n",
            "    [X2 <= 11.0000]\n",
            "    Left:\n",
            "      Leaf: 1\n",
            "    Right:\n",
            "      Leaf: 1\n",
            "  Right:\n",
            "    Leaf: 0\n",
            "\n",
            "Predictions: [0 0]\n"
          ]
        }
      ]
    }
  ]
}